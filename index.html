<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load("jquery", "1.3.2");
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"
></script>
<link
  href="https://fonts.googleapis.com/css2?family=Lato&display=swap"
  rel="stylesheet"
/>
<link
  href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  type="text/css"
  href="./resources/style.css"
  media="screen"
/>

<html lang="en">
  <head>
    <title>Hybrid Inverse Reinforcement Learning</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
    <meta property="og:title" content="Hybrid Inverse Reinforcement Learning" />
    <meta
      property="og:description"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines
      across a wide set of continuous control environments."
    />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta
      property="twitter:card"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines empirically
      across a wide set of continuous control environments."
    />
    <meta
      property="twitter:title"
      content="Inverse Reinforcement Learning without Reinforcement Learning"
    />
    <meta
      property="twitter:description"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines empirically
      across a wide set of continuous control environments."
    />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  </head>

  <body>
    <div class="container">
      <div class="title">
        Hybrid Inverse Reinforcement Learning
      </div>

      <div class="venue">
        ICML '24
      </div>

      <br /><br />

      <div class="author">
        <a href="https://jren03.github.io/">Juntao Ren</a><sup>*, 2</sup>
      </div>
      <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>*, 1</sup>
      </div>
      <div class="author">
        <a href="https://zstevenwu.com/">Steven Wu</a><sup>1</sup>
      </div>
      <br />
      <div class="author">
        <a href="https://www.ri.cmu.edu/ri-faculty/j-andrew-drew-bagnell/"
          >Drew Bagnell</a
        ><sup>1, 3</sup>
      </div>
      <div class="author">
        <a href="http://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a
        ><sup>2, 3</sup>
      </div>

      <br /><br />

      <div class="affiliation"><sup>1&nbsp;</sup>CMU</div>
      <div class="affiliation"><sup>2&nbsp;</sup>Cornell University</div>
      <div class="affiliation"><sup>3&nbsp;</sup>Aurora Innovation</div>

      <br />
      <div class="footnote"><sup>*</sup>Indicates equal contribution.</div>

      <br /><br />

      <div class="links">
        <a href="https://arxiv.org/pdf/2402.08848"
          ><i
            class="fa fa-file-text"
            ,
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Paper]</a
        >
      </div>

      <div class="links">
        <a
          href="https://youtu.be/zdbhPZ0Epow?si=QGXpMtNJ5O1pmg3h?start=5:30&end=38:30"
          ><i
            class="fa fa-play-circle"
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Video]</a
        >
      </div>

      <div class="links">
        <a href="https://github.com/jren03/garage/tree/main"
          ><i
            class="fa fa-github"
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Code]</a
        >
      </div>

      <br /><br />

      <img
        style="width: 50%;"
        src="./resources/hyper_ffig.svg"
        alt="Teaser figure."
      />
      <br />
      <p style="width: 80%;">
        <br />
        We present a general theoretical framework for designing efficient
        algorithms for interactive imitation learning. We leverage this
        framework to derive novel inverse reinforcement learning algorithms (one
        model-free, one model-based), both of which come with strong performance
        guarantees and compare favorably to multiple baselines empirically
        across a wide set of continuous control environments.
      </p>
      <hr />

      <h1>Abstract</h1>
      <p style="width: 80%;">
        The inverse reinforcement learning approach to imitation learning is a
        double-edged sword. On the one hand, it can enable learning from a
        smaller number of expert demonstrations with more robustness to error
        compounding than behavioral cloning approaches. On the other hand, it
        requires that the learner repeatedly solve a computationally expensive
        reinforcement learning (RL) problem. Often, much of this computation is
        wasted searching over policies very dissimilar to the expert's. In this
        work, we propose using <i>hybrid RL</i> &mdash; training on a mixture of
        online and expert data &mdash; to curtail unnecessary exploration.
        Intuitively, the expert data focuses the learner on good states during
        training, which reduces the amount of exploration required to compute a
        strong policy. Notably, such an approach doesn't need the ability to
        reset the learner to arbitrary states in the environment, a requirement
        of prior work in efficient inverse RL. More formally, we derive a
        reduction from inverse RL to <i>expert-competitive RL</i> (rather than
        globally optimal RL) that allows us to dramatically reduce interaction
        during the inner policy search loop while maintaining the benefits of
        the IRL approach. This allows us to derive both model-free and
        model-based hybrid inverse RL algorithms with strong policy performance
        guarantees. Empirically, we find that our approaches are significantly
        more sample efficient than standard inverse RL and several other
        baselines that require stronger assumptions on a suite of continuous
        control tasks.
      </p>
      <br />

      <hr />
      <h1>Video</h1>
      <div class="video-container">
        <iframe
          src="https://youtu.be/zdbhPZ0Epow?si=QGXpMtNJ5O1pmg3h?start=5:30&end=38:30"
          frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div>

      <br />

      <hr />

      <h1>Key Insights</h1>

      <h2>1. Inverse IRL is Inefficient Due to RL</h2>
      <p style="width: 80%;">
        Traditional inverse RL (IRL) is formulated as a two-player game with an
        adversary selecting a reward function in the outer loop, and a learner
        that optimizes this reward function <b><i>to completion</i></b> via
        reinforcement learning in the inner loop. Thus, the majority of time in
        IRL is spent in this inner loop searching over policies. However,
        because the expert doesn't need to be the optimal policy under the
        adversarially chosen reward, this can lead to a lot of wasted
        computation, as shown in the two trees below:
      </p>
      <img style="width: 60%" src="./resources/ecr.svg" />
      <br />
      <br />
      <p style="width: 80%;">
        Note in the above tree MDP that the
        <span class="expert">expert</span> is not optimal under reward $f_1$ or
        $f_2$. Nevertheless, in computing the <i>best response</i> to the reward
        chosen in the outer loop, the <span class="learner">learner</span> needs
        to pay for the worst-case exploration complexity of RL &#8212;
        $O(\exp(H))$. Given our goal is merely to imitate the expert, this
        search procedure is needlessly expensive. Furthermore, given the
        best-response can change widely across iterations, this sort of
        procedure can introduce instability into the training process. The same
        set of concerns apply to algorithms that take a small no-regret step
        (e.g. GAIL) at each inner loop iteration: they need to explore the
        entire MDP at least once.
      </p>

      <h2>2. Reducing Inverse RL to Expert Competitive RL</h2>
      <p style="width: 80%;">
        Recall that at heart, what we care about in IRL is competing with the
        expert. Thus, intuitively, as long as we can consistently compete with
        the expert in the inner loop &#8212; i.e. via an
        <i>expert-competitive response</i> rather than a best-response &#8212;
        we might hope that we can derive algorithms with strong performance
        guarantees. We can now formalize this intuition via a simple proof. We
        first define an <b>Expert Relative Regret Oracle</b> (ERROr).
      </p>
      <p style="width: 80%;">
        Suppose we have some policy-selection algorithm $\mathbb{A}_{\pi}$ which
        produces a sequence of policies $\pi_{t+1} = \mathbb{A}_{\pi}(f_{1:t})$
        for any sequence of reward functions $f_{1:t}$, we say the algorithm
        satisfies the $\mathsf{Reg}_{\pi}(T)$ expert-relative regret guarantee
        if $$\sum_{t=1}^T J(\pi_E, f_t) - J(\pi_t, f_t) \leq
        \mathsf{Reg}_{\pi}(T).$$ In words, this equation tells us that no matter
        what sequence of reward functions we face, on average, we perform almost
        as well as the expert. If you have an RL Theory background, this sort of
        <i>performance difference</i> should set of your spidey senses: most RL
        algorithms operate by minimizing upper bounds of it. We'll go through
        some examples of such algorithms in a bit but first, let us see how we
        can use an ERROr algorithm inside IRL. Let's assume we pick a sequence
        of reward functions $f_{1:T}$ via training a discriminator between
        learner and expert data. Then, via the ERROr property, we have that
        $\bar{\pi}$ (the mixture of $\pi_{1:T}$) satisfies
        $$\begin{aligned}J(\pi_E, r) - J(\bar{\pi}, r) &= \frac{1}{T}
        \sum_{t=1}^T J(\pi_E, r) - J(\pi_t, r) \\ &\leq \max_{f^{\star} \in
        \mathit{F}_r} \frac{1}{T} \sum_{t=1}^T J(\pi_E, f^{\star}) - J(\pi_t,
        f^{\star}) \\ &\leq \frac{1}{T} \sum_{t=1}^T J(\pi_E, f_t) - J(\pi_t,
        f_t) + \frac{\mathsf{Reg}_f(T)}{T}H \\ &\leq
        \frac{\mathsf{Reg}_{\pi}(T)}{T} + \frac{\mathsf{Reg}_f(T)}{T}
        H.\end{aligned}$$ If we use a no-regret reward selection algorithm such
        as online gradient descent, the second term goes to 0 as $T \to \infty$.
        In short,
        <i
          >we only need to compete with the expert on average to ensure we learn
          a policy with strong performance.</i
        >
      </p>

      <h2>3. Using Hybrid RL to Perform ECRs</h2>
      <p style="width: 80%;">
        By leveraging prior algorithms in <i><b>hybrid RL</b></i> (<a
          href="https://arxiv.org/abs/1203.1007"
          >Ross and Bagnell (2012)</a
        >, <a href="https://arxiv.org/abs/2210.06718">Song et al. (2023)</a>,
        <a href="https://proceedings.mlr.press/v202/vemula23a.html"
          >Vemula et al. (2023)</a
        >) that satisfy the ERROr property in the inner RL loop, we can
        efficiently find strong policies without reward information. In
        particular, we derive two <i><b>hybrid</b></i> inverse RL algorithms
        with strong performance guarantees using this framework: a model-free
        variant named <code>HyPE</code> (Hybrid Policy Emulation) and a
        model-based variant named <code>HyPER</code> (Hybrid Policy Emulation
        with Resets).
      </p>

      <p style="width: 80%">
        For example, we instantiate <code>HyPE</code> using the HyQ algorithm
        from
        <a href="https://arxiv.org/abs/2210.06718">Song et al. (2023)</a> as the
        inner RL algorithm as follows:
      </p>
      <div style="display: flex; justify-content: center;">
        <ol style="width: 70%; text-align: left;">
          <li>
            Maintain one buffer of expert samples, one buffer of policy
            rollouts.
          </li>
          <li>
            Run your on/off-policy RL algorithm on the mixture of these buffers.
          </li>
        </ol>
      </div>

      <p style="width: 80%">
        Correspondingly, we instantiate <code>HyPER</code> using the LAMPS
        algorithm from
        <a href="https://proceedings.mlr.press/v202/vemula23a.html"
          >Vemula et al. (2023)</a
        >. We note that <code>HyPER</code> is equivalent to running the
        <a href="https://gokul.dev/filter/">FILTER</a>
        algorithm as follows:
      </p>
      <div style="display: flex; justify-content: center;">
        <ol style="width: 70%; text-align: left;">
          <li>
            Rollout the policy in the real world.
          </li>
          <li>
            Update the model on a mixture of learner and expert data.
          </li>
          <li>
            Run some RL algorithm <i>inside</i> the model, resetting to states
            from the expert demonstrations to reduce exploration and model
            exploitation.
          </li>
        </ol>
      </div>
      <h2>4. Empirical Benchmark</h2>
      <p style="width: 80%">
        We benchmark <code>HyPE</code> and <code>HyPER</code> against other
        inverse RL baselines on locomotion tasks from D4RL and the MuJoCo suite.
        Notably, <i>neither</i> of our algorithms requires the ability to reset
        to specific states within the environment. To the best of our knowledge,
        this is the highest performance achieved by an inverse RL algorithm on
        <code>antmaze</code>, including those that require generative model
        access to the environment.
      </p>
      <img style="width: 60%" src="./resources/maze_shared.svg" />
      <p style="width: 80%">
        We release all our code in the link below.
      </p>
      <a href="https://github.com/jren03/garage/tree/main"
        ><i
          class="fa fa-github"
          style="font-size: 50px; padding-bottom: 10px"
        ></i
        ><br />[Code]</a
      >
      <hr />

      <h1>Paper</h1>
      <div class="paper-thumbnail">
        <a href="https://arxiv.org/pdf/2402.08848">
          <img
            class="layered-paper-big"
            width="100%"
            src="./resources/paper.svg"
            alt="Paper thumbnail"
          />
        </a>
      </div>
      <div class="paper-info">
        <h3>Hyrbid Inverse Reinforcement Learning</h3>
        <p>
          Juntao Ren<sup>*</sup>, Gokul Swamy<sup>*</sup>, Zhiwei Steven Wu, J.
          Andrew Bagnell, Sanjiban Choudhury
        </p>
        <pre><code>@misc{ren2024hybrid,
    title = {Hybrid Inverse Reinforcement Learning},
    author = {Juntao Ren and Gokul Swamy and Zhiwei Steven Wu and J. Andrew Bagnell and Sanjiban Choudhury},
    year = {2024},
    booktitle = {International Conference on Machine Learning},
}</code></pre>
      </div>

      <br />
      <hr />

      <h1>Acknowledgements</h1>
      <p style="width: 80%;">
        This template was originally made by
        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and
        <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV
        project, and adapted to be mobile responsive by
        <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a
        >. The code we built on can be found
        <a href="https://github.com/elliottwu/webpage-template">here</a>.
      </p>

      <br />
    </div>

    <!-- To open links in new tab -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
      $(document).ready(function() {
        $("a").attr({
          target: "_blank",
          rel: "noopener noreferrer",
        });
      });
    </script>
  </body>
</html>
