<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load("jquery", "1.3.2");
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>
<script
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"
></script>
<link
  href="https://fonts.googleapis.com/css2?family=Lato&display=swap"
  rel="stylesheet"
/>
<link
  href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  type="text/css"
  href="./resources/style.css"
  media="screen"
/>

<html lang="en">
  <head>
    <title>Hybrid Inverse Reinforcement Learning</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
    <meta property="og:title" content="Hybrid Inverse Reinforcement Learning" />
    <meta
      property="og:description"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines
      across a wide set of continuous control environments."
    />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta
      property="twitter:card"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines empirically
      across a wide set of continuous control environments."
    />
    <meta
      property="twitter:title"
      content="Inverse Reinforcement Learning without Reinforcement Learning"
    />
    <meta
      property="twitter:description"
      content="We present a general theoretical framework for designing efficient
      algorithms for interactive imitation learning. We leverage this
      framework to derive novel inverse reinforcement learning algorithms (one
      model-free, one model-based), both of which come with strong performance
      guarantees and compare favorably to multiple baselines empirically
      across a wide set of continuous control environments."
    />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  </head>

  <body>
    <div class="container">
      <div class="title">
        Hybrid Inverse Reinforcement Learning
      </div>

      <div class="venue">
        ICML '24
      </div>

      <br /><br />

      <div class="author">
        <a href="https://gokul.dev/">Juntao Ren</a><sup>*, 2</sup>
      </div>
      <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>*, 1</sup>
      </div>
      <div class="author">
        <a href="https://zstevenwu.com/">Steven Wu</a><sup>1</sup>
      </div>
      <div class="author">
        <a href="https://www.ri.cmu.edu/ri-people/j-andrew-drew-bagnell/"
          >Drew Bagnell</a
        ><sup>3, 1</sup>
      </div>
      <div class="author">
        <a href="http://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a
        ><sup>2</sup>
      </div>

      <br /><br />

      <div class="affiliation"><sup>1&nbsp;</sup>CMU</div>
      <div class="affiliation"><sup>2&nbsp;</sup>Cornell University</div>
      <div class="affiliation"><sup>3&nbsp;</sup>Aurora Innovation</div>

      <br /><br />

      <div class="links">
        <a href="https://arxiv.org/pdf/2402.08848"
          ><i
            class="fa fa-file-text"
            ,
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Paper]</a
        >
      </div>
      <!-- TODO -->
      <!-- <div class="links">
        <a href="https://youtu.be/UATgep5bT8s"
          ><i
            class="fa fa-play-circle"
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Video]</a
        >
      </div> -->
      <!-- TODO -->
      <div class="links">
        <a href="https://github.com/jren03/garage/tree/main"
          ><i
            class="fa fa-github"
            style="font-size: 50px; padding-bottom: 10px"
          ></i
          ><br />[Code]</a
        >
      </div>

      <br /><br />

      <img
        style="width: 60%;"
        src="./resources/hyper_ffig.svg"
        alt="Teaser figure."
      />
      <br />
      <p style="width: 80%;">
        <br />
        We present a general theoretical framework for designing efficient
        algorithms for interactive imitation learning. We leverage this
        framework to derive novel inverse reinforcement learning algorithms (one
        model-free, one model-based), both of which come with strong performance
        guarantees and compare favorably to multiple baselines empirically
        across a wide set of continuous control environments.
      </p>
      <hr />

      <h1>Abstract</h1>
      <p style="width: 80%;">
        The inverse reinforcement learning approach to imitation learning is a
        double-edged sword. On the one hand, it can enable learning from a
        smaller number of expert demonstrations with more robustness to error
        compounding than Behavioral Cloning approaches. On the other hand, it
        requires that the learner repeatedly solve a computationally expensive
        reinforcement learning (RL) problem. Often, much of this computation is
        wasted searching over policies very dissimilar to the expert's. In this
        work, we propose using <i>hybrid RL</i> &mdash; training on a mixture of
        online and expert data &mdash; to curtail unnecessary exploration.
        Intuitively, the expert data focuses the learner on good states during
        training, which reduces the amount of exploration required to compute a
        strong policy. Notably, such an approach doesn't need the ability to
        reset the learner to arbitrary states in the environment, a requirement
        of prior work in efficient inverse RL. More formally, we derive a
        reduction from inverse RL to <i>expert-competitive RL</i> (rather than
        globally optimal RL) that allows us to dramatically reduce interaction
        during the inner policy search loop while maintaining the benefits of
        the IRL approach. This allows us to derive both model-free and
        model-based hybrid inverse RL algorithms with strong policy performance
        guarantees. Empirically, we find that our approaches are significantly
        more sample efficient than standard inverse RL and several other
        baselines that require stronger assumptions on a suite of continuous
        control tasks.
      </p>
      <br />

      <!-- TODO -->
      <!-- <hr /> -->
      <!-- <h1>Video</h1>
      <div class="video-container">
        <iframe
          src="https://www.youtube.com/embed/UATgep5bT8s"
          frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen
        ></iframe>
      </div> -->
      <!-- TODO -->
      <br />

      <hr />

      <h1>Key Insights</h1>

      <h2>1. Reducing inverse RL to expert-competitive RL</h2>
      <p style="width: 80%;">
        Traditional inverse RL is formulated as two-player game with an
        adversarial discriminator that select reward function in the outer loop,
        and a learner that optimizes this reward function via reinforcement
        learning in the inner loop. The learner will have discovered a good
        enough policy if, for any reward function chosen by the adversary, it
        performs as well as the expert. However, typical online RL algorithms
        used in this inner loop sees the learner spending the majority of
        interactions trying out policies that are quite dissimilar to the
        expertâ€™s in the hope of finding a bit of reward. Visually,
      </p>
      <img style="width: 60%" src="./resources/ecr.svg" />
      <br />
      <br />
      <p style="width: 80%;">
        note in the above tree MDP that the
        <span class="expert">expert</span> is not optimal under reward $f_1$ or
        $f_2$. However, the <span class="learner">learner </span> still searches
        through the entire tree to find the <i>best</i> policy under the
        proposed reward function. Given our goal is merely to imitate the
        expert, this search procedure is needlessly expensive. Further, because
        the <i>best-response</i> policy can vary wildly across iterations, this
        can introduce instability into the training process. Instead, one might
        hope that as long as the learner can consistently compete with the
        <i>expert</i> under whatever metric the adversary chooses, we should be
        able to guarantee that we compete with the expert under the ground-truth
        reward.
      </p>

      <h2>2. Formalizing the intuition</h2>
      <p style="width: 80%;">
        Suppose we have a <i>no-regret</i> reward selection algorithm
        $\mathbb{A}_{f}$ which produces iterates $f_{t+1} =
        \mathbb{A}_f(\ell_{1:t})$, where $\ell_t(f) = \frac{1}{H}\left( J(\pi_t,
        f) - J(\pi_E, f) \right)$, such that $$\sum_{t=1}^T \ell_t(f_t) -
        \min_{f^{\star} \in \mathit{F}_r} \sum_{t=1}^T \ell_t(f^{\star}) \leq
        \mathsf{Reg}_f(T),$$ with $\lim_{t \to \infty}
        \frac{\mathsf{Reg}_f(T)}{T} = 0$. Now, given some policy-selection
        algorithm $\mathbb{A}_{\pi}$ which produces a sequence of policies
        $\pi_{t+1} = \mathbb{A}_{\pi}(f_{1:t})$ for any sequence of reward
        functions $f_{1:t}$, we say the algorithm satisfies the
        $\mathsf{Reg}_{\pi}(T)$ expert-relative regret guarantee if
        $$\sum_{t=1}^T J(\pi_E, f_t) - J(\pi_t, f_t) \leq
        \mathsf{Reg}_{\pi}(T).$$ Then $\bar{\pi}$ (the mixture of $\pi_{1:T}$)
        satisfies $$\begin{aligned}J(\pi_E, r) - J(\bar{\pi}, r) &= \frac{1}{T}
        \sum_{t=1}^T J(\pi_E, r) - J(\pi_t, r) \\ &\leq \max_{f^{\star} \in
        \mathit{F}_r} \frac{1}{T} \sum_{t=1}^T J(\pi_E, f^{\star}) - J(\pi_t,
        f^{\star}) \\ &\leq \frac{1}{T} \sum_{t=1}^T J(\pi_E, f_t) - J(\pi_t,
        f_t) + \frac{\mathsf{Reg}_f(T)}{T}H \\ &\leq
        \frac{\mathsf{Reg}_{\pi}(T)}{T} + \frac{\mathsf{Reg}_f(T)}{T}
        H.\end{aligned}$$ Since $\mathbb{A}_f$ is no-regret, the second term
        goes to 0 as $T \to \infty$, meaning we only need to compete with the
        expert on average to ensure we learn a policy with strong performance.
      </p>

      <h2>3. Practical Procedure</h2>
      <p style="width: 80%;">
        To this end, we provide a general framework that allows us to lift any
        off-the-shelf RL algorithm that guarantees competing with the expert to
        the space of inverse RL and inherit the performance guarantees therein.
        We derive two inverse RL algorithms with strong performance guarantees
        using this framework: a model-free variant named
        <code>HyPE</code> (Hybrid Policy Emulation) and a model-based variant
        named <code>HyPER</code> (Hybrid Policy Emulation with Resets).
      </p>

      <p style="width: 80%">
        For example, we instantiate <code>HyPE</code> using the HyQ algorithm
        from
        <a href="https://arxiv.org/abs/2210.06718">Song et al. (2023)</a> as the
        inner RL algorithm as follows:
      </p>
      <div style="display: flex; justify-content: center;">
        <ol style="width: 70%; text-align: left;">
          <li>
            Maintain one buffer of expert samples, one buffer of policy
            rollouts.
          </li>
          <li>
            Update the policy (and critic network if using off-policy RL) using
            a mixture of samples from both buffers.
          </li>
        </ol>
      </div>

      <p style="width: 80%">
        Correspondingly, we instantiate <code>HyPER</code> using the LAMPS
        algorithm from
        <a href="https://proceedings.mlr.press/v202/vemula23a.html"
          >Vemula et al. (2023)</a
        >
        as follows:
      </p>
      <div style="display: flex; justify-content: center;">
        <ol style="width: 70%; text-align: left;">
          <li>
            Maintain separate buffers for expert samples, policy rollouts, and
            model rollouts.
          </li>
          <li>
            Update the model using a mixture of samples from expert and policy
            buffers.
          </li>
          <li>
            Reset the agent to a mixture of expert and policy states within the
            model.
          </li>
          <li>Update networks on rollout states within the model.</li>
        </ol>
      </div>

      <h2>4. Empirical Benchmark</h2>
      <p style="width: 80%">
        We benchmark <code>HyPE</code> and <code>HyPER</code> against other
        inverse RL baselines on locomotion tasks from D4RL and the MuJoCo suite.
        Notably, <i>neither</i> of our algorithms require the ability to reset
        to specific states within the environment. To the best of our knowledge,
        this is the highest performance achieved by an inverse RL algorithm on
        the <code>antmaze</code>, including those that require generative model
        access to the environment.
      </p>
      <img style="width: 60%" src="./resources/maze_shared.svg" />
      <p style="width: 80%">
        We release all our code in the link below.
      </p>
      <a href="https://github.com/jren03/garage/tree/main"
        ><i
          class="fa fa-github"
          style="font-size: 50px; padding-bottom: 10px"
        ></i
        ><br />[Code]</a
      >
      <hr />

      <h1>Paper</h1>
      <div class="paper-thumbnail">
        <a href="https://arxiv.org/pdf/2402.08848">
          <img
            class="layered-paper-big"
            width="100%"
            src="./resources/paper.svg"
            alt="Paper thumbnail"
          />
        </a>
      </div>
      <div class="paper-info">
        <h3>Hyrbid Inverse Reinforcement Learning</h3>
        <p>
          Juntao Ren<sup>*</sup>, Gokul Swamy<sup>*</sup>, J. Andrew Bagnell,
          Zhiwei Steven Wu, Sanjiban Choudhury
        </p>
        <pre><code>@misc{ren2024hybrid,
    title = {Hybrid Inverse Reinforcement Learning},
    author = {Juntao Ren and Gokul Swamy and Zhiwei Steven Wu and J. Andrew Bagnell and Sanjiban Choudhury},
    year = {2024},
    booktitle = {International Conference on Machine Learning},
}</code></pre>
      </div>

      <br />
      <hr />

      <h1>Acknowledgements</h1>
      <p style="width: 80%;">
        This template was originally made by
        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and
        <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV
        project, and adapted to be mobile responsive by
        <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a
        >. The code we built on can be found
        <a href="https://github.com/elliottwu/webpage-template">here</a>.
      </p>

      <br />
    </div>

    <!-- To open links in new tab -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script>
      $(document).ready(function() {
        $("a").attr({
          target: "_blank",
          rel: "noopener noreferrer",
        });
      });
    </script>
  </body>
</html>
